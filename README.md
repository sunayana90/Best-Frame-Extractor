ğŸ¥ Best Frame Extractor from Video

A FastAPI-based computer vision application that automatically extracts the best quality image from a video by analyzing facial attributes such as mouth state, eye openness, motion stability, sharpness, and face position.

The system intelligently selects frames during pause moments (when the face is stable), ensuring natural and professional-looking results.

ğŸš€ Features

ğŸ“¤ Upload any video through a clean web UI

ğŸ¯ Adaptive frame extraction (captures more frames during pauses)

ğŸ˜€ Face detection using MediaPipe

ğŸ‘„ Mouth closed detection (rejects talking frames)

ğŸ‘ï¸ Eye-open detection (rejects blink frames)

ğŸ” Blur detection (prefers sharp images)

ğŸ¯ Face-centered scoring

ğŸ–¼ï¸ Automatically saves and displays the best frame

ğŸŒ Simple, professional frontend using HTML + CSS

ğŸ§  How It Works (High-Level)

Video Upload
User uploads a video via the browser UI.

Adaptive Frame Extraction

Normal motion â†’ sample fewer frames

Low motion (pause) â†’ capture more frames

Face & Landmark Analysis
Each frame is analyzed using MediaPipe:

Face detection

Facial landmarks

Quality Filters

Mouth must be closed

Eyes must be open

Image must be sharp

Face should be centered

Scoring System
Frames are scored based on:

Sharpness

Facial stability

Mouth state

Eye openness

Face position

Best Frame Selection
The frame with the highest score is saved and displayed as output.

ğŸ› ï¸ Tech Stack

Backend: FastAPI

Computer Vision: OpenCV, MediaPipe

Frontend: HTML, CSS, JavaScript

Server: Uvicorn

Language: Python 3.9+

ğŸ“ Project Structure
best-frame-extractor/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI routes & server
â”‚   â”œâ”€â”€ video_utils.py       # Adaptive frame extraction logic
â”‚   â””â”€â”€ frame_selector.py    # Face analysis & scoring logic
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html           # Frontend UI
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css            # UI styling
â”‚
â”œâ”€â”€ uploads/                 # Uploaded videos
â”œâ”€â”€ outputs/                 # Extracted best frame
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone <your-repo-url>
cd best-frame-extractor

2ï¸âƒ£ Create Virtual Environment
python -m venv venv


Activate it:

# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

â–¶ï¸ Run the Application
uvicorn app.main:app --reload


Open in browser:

http://127.0.0.1:8000

ğŸ§ª How to Use

Open the web app

Upload a video (talking or recorded selfie video)

Click Extract Best Frame

The system automatically selects and displays the best image

ğŸ“¸ Example Use Cases

Interview profile photo extraction

YouTube / content creator thumbnails

Online meeting profile pictures

Resume / portfolio photos

Automated photo capture systems

ğŸ“ˆ Improvements Implemented

Adaptive frame sampling based on motion detection

Scale-invariant mouth closed detection

Eye blink rejection using facial landmarks

Interpretable scoring logic

Clean separation of backend and UI

ğŸ”® Future Enhancements

Top-K best frame selection

Consecutive-frame stability (temporal smoothing)

Smile / neutral expression classification

Progress bar during processing

Download button for extracted image

Cloud deployment (Render / AWS)

ğŸ§  Learning Outcomes

Practical use of MediaPipe Face Mesh

Real-world video frame analysis

Facial landmarkâ€“based quality checks

Designing interpretable scoring systems

Building a full ML-powered web application